# Notes on data validation {#validation}

- Why data validation is important
- Compare to testing and TDD in dev
- Different purposes:
  - Monitor source data quality -> react to exceptions, feedback loop
  - Prevent critically invalid data from being saved, e.g., duplicates over primary keys
  - Mark data that is only partially OK - no need to discard it, if can be used for *some* purposes
    - E.g., door status missing
  - Find data that can be corrected afterwards
    - E.g., stop matching
  - Main point: do not discard invalid data blindly
- R `{validate}` and GTFS as example
  - Limited scale - not wanting to try with 100 M rows of HFP data
- HFP data from a week: a PostgreSQL example
  - Considerations of how *not* to do it
    - On-the-fly validation with functions
    - Saving validation results to separate fields
    - Saving validation labels to separate table
  - My suggestion: use a bitstring field
    - Part of the table itself, or as separate table, if invalid records are sparse
    - Demonstration with some rules
    - Pros: space-efficient, quick to query and aggregate, shows correlated rules directly
    - Cons: needs encoding and decoding, not very common practice, adds complexity
  - Beyond per-record validations: more complex examples
    - Successive records (e.g., speed)
    - Successive records of certain event types (e.g., alternation of DOO and DOC)
    - Validation by journey
    - Validation by vehicle (need to select time window, e.g., date)

\newpage
